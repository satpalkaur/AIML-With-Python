{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNryZZQfvD/DWSSCG78iNbp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/satpalkaur/AIML-With-Python/blob/main/NLP%20Example1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8soeRILClW3_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re                           #Imports the regular expression module, which provides tools for text pattern matching and manipulation (used here to remove punctuation).\n",
        "import nltk                         #Imports the Natural Language Toolkit (NLTK) to provide tools for tokenization, lemmatization, and more.\n",
        "from nltk.corpus import stopwords   #Imports the stopwords corpus from NLTK, which contains a list of common words (such as \"the\", \"and\", \"in\") that are often removed in text preprocessing because they do not carry significant meaning.\n",
        "from nltk.tokenize import word_tokenize   #Imports the word_tokenize function from NLTK, which splits a text into individual words (tokens).\n",
        "from nltk.stem import WordNetLemmatizer   #Imports the WordNetLemmatizer class from NLTK, which is used to reduce words to their base or root form (lemmatization)."
      ],
      "metadata": {
        "id": "k3FYl0LOnjjX"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK resources (run only once)\n",
        "nltk.download('punkt')       #Downloads the Punkt tokenizer models, which are necessary for tokenization (splitting text into words or sentences).\n",
        "nltk.download('stopwords')   #Downloads the list of stopwords in various languages, including English. This list is used to filter out common, non-informative words.\n",
        "nltk.download('wordnet')     #Downloads the WordNet lexicon, a large lexical database of English, which is used for lemmatization. It helps the lemmatizer determine the base form of words.\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zayn9p0znsHD",
        "outputId": "8892f434-54b9-4933-add7-458c95782d35"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example text\n",
        "text = \"This is an example sentence, to demonstrate text preprocessing! We'll clean it and tokenize.\"\n",
        "\n",
        "#The text contains punctuation, capital letters, and stopwords, making it a good candidate for cleaning.\n",
        "\n",
        "# Function for text preprocessing\n",
        "def preprocess_text(text):           #Defines a function preprocess_text that takes a string text as input and performs several preprocessing steps (like lowercasing, tokenization, etc.).\n",
        "    # 1. Lowercase the text\n",
        "    text = text.lower()              #Converts the entire text to lowercase to ensure uniformity, as text processing often ignores case sensitivity. For example, \"Text\" and \"text\" should be treated as the same word.\n",
        "\n",
        "    # 2. Remove punctuation and special characters\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)      #Uses the re.sub function to remove punctuation and special characters. The regular expression r'[^\\w\\s]' matches any character that is not a word character (\\w) or a whitespace (\\s). This removes things like commas, periods, and exclamation marks.\n",
        "\n",
        "    # 3. Tokenize the text\n",
        "    tokens = word_tokenize(text)        #This splits the text into individual words, resulting in a list of tokens.\n",
        "\n",
        "    # 4. Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))   #Loads a set of stopwords from the NLTK library for the English language.\n",
        "    tokens = [word for word in tokens if word not in stop_words]   #Filters out the stopwords from the tokenized text.\n",
        "\n",
        "    # 5. Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()  #Creates an instance of the WordNetLemmatizer.\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]    #Applies lemmatization to each token in the list.\n",
        "\n",
        "    return tokens      #Returns the list of preprocessed tokens (cleaned, tokenized, stopword-free, and lemmatized) as the output of the function.\n",
        "    # Process the example text\n",
        "cleaned_text = preprocess_text(text)     #Calls the preprocess_text function on the example text. This applies all the preprocessing steps defined earlier, resulting in a cleaned list of tokens.\n",
        "print(\"Cleaned Tokens:\", cleaned_text)   #Prints the cleaned tokens to the console, showing the final result of the preprocessing steps.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0a0E5rQorp-",
        "outputId": "9ee96d8d-f126-4ab2-f711-4113b84dc561"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned Tokens: ['example', 'sentence', 'demonstrate', 'text', 'preprocessing', 'well', 'clean', 'tokenize']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy                         #This imports the spaCy library, which is an advanced NLP library for Python. It provides tools for tokenization, part-of-speech tagging, named entity recognition, dependency parsing, and more.\n",
        "\n",
        "# Load spaCy's English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")    #This loads the small English language model (en_core_web_sm) provided by spaCy. en_core_web_sm is the smallest English model, optimized for speed rather than accuracy.\n",
        "\n",
        "# Sample text\n",
        "text = \"Apple is looking at buying U.K. startup for $1 billion. Steve Jobs founded Apple in 1976.\"\n",
        "\n",
        "#It contains a few named entities (like \"Apple\" and \"Steve Jobs\"), as well as some financial information (like \"$1 billion\") and dates.\n",
        "\n",
        "# Process the text using spaCy\n",
        "doc = nlp(text)     #nlp(text) processes the input text and returns a Doc object, which is a container for the processed text.\n",
        "\n",
        "# Part-of-Speech (POS) tagging\n",
        "print(\"Part-of-Speech Tagging:\")  #Part-of-speech (POS) tagging is the task of assigning a grammatical category (like noun, verb, adjective) to each token in the text.\n",
        "for token in doc:\n",
        "    print(f\"{token.text}: {token.pos_} ({token.tag_})\")  #token.text: This gives the original text of each token (word). token.pos_: This gives the coarse-grained part-of-speech label (e.g., NOUN, VERB, ADJ). token.tag_: This provides a more fine-grained POS tag (e.g., NN for singular noun, VBD for past-tense verb, etc.).\n",
        "\n",
        "# Named Entity Recognition (NER)\n",
        "print(\"\\nNamed Entities:\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"{ent.text}: {ent.label_} ({spacy.explain(ent.label_)})\") #doc.ents: This contains a list of all the named entities detected in the processed text. ent.text: This gives the text of the named entity (e.g., \"Apple\", \"$1 billion\"). ent.label_: This gives the label for the entity (e.g., ORG for organizations, GPE for geopolitical entities, DATE for dates, MONEY for monetary values). spacy.explain(ent.label_): This provides an explanation of the entity label."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nk9VFSy7qOSl",
        "outputId": "11087dee-9604-4ce1-9e96-6e4699d43817"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Part-of-Speech Tagging:\n",
            "Apple: PROPN (NNP)\n",
            "is: AUX (VBZ)\n",
            "looking: VERB (VBG)\n",
            "at: ADP (IN)\n",
            "buying: VERB (VBG)\n",
            "U.K.: PROPN (NNP)\n",
            "startup: VERB (VBD)\n",
            "for: ADP (IN)\n",
            "$: SYM ($)\n",
            "1: NUM (CD)\n",
            "billion: NUM (CD)\n",
            ".: PUNCT (.)\n",
            "Steve: PROPN (NNP)\n",
            "Jobs: PROPN (NNP)\n",
            "founded: VERB (VBD)\n",
            "Apple: PROPN (NNP)\n",
            "in: ADP (IN)\n",
            "1976: NUM (CD)\n",
            ".: PUNCT (.)\n",
            "\n",
            "Named Entities:\n",
            "Apple: ORG (Companies, agencies, institutions, etc.)\n",
            "U.K.: GPE (Countries, cities, states)\n",
            "$1 billion: MONEY (Monetary values, including unit)\n",
            "Steve Jobs: PERSON (People, including fictional)\n",
            "Apple: ORG (Companies, agencies, institutions, etc.)\n",
            "1976: DATE (Absolute or relative dates or periods)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "# Sample text\n",
        "text = \"I absolutely love this product! It's fantastic and works like a charm.\"\n",
        "\n",
        "# Create a TextBlob object\n",
        "blob = TextBlob(text)\n",
        "\n",
        "# Perform sentiment analysis\n",
        "sentiment = blob.sentiment\n",
        "\n",
        "# Output sentiment polarity and subjectivity\n",
        "print(f\"Polarity: {sentiment.polarity}\")  # Polarity: -1 (negative) to 1 (positive)\n",
        "print(f\"Subjectivity: {sentiment.subjectivity}\")  # Subjectivity: 0 (objective) to 1 (subjective)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QDqoe4WwKrp",
        "outputId": "c6a53768-12d4-4e7d-9ec7-702293efd06c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Polarity: 0.5125\n",
            "Subjectivity: 0.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "import gensim                               #Imports the gensim library, which is a popular Python library for topic modeling, document similarity, and vector space modeling. It includes the LDA model, which we’ll use for topic modeling.\n",
        "from gensim import corpora                  #Imports the corpora module from Gensim, which provides utilities for handling a corpus of documents. It includes methods for creating a dictionary (mapping words to unique IDs) and for creating document-term matrices.\n",
        "from gensim.models import LdaModel          #Imports the LdaModel class from Gensim. This class is used for training an LDA model on a corpus to discover topics within a collection of documents.\n",
        "from nltk.corpus import stopwords           #Imports the stopwords corpus from NLTK (Natural Language Toolkit), which contains a list of common words (e.g., \"the\", \"is\", \"and\") that are often removed from text during preprocessing.\n",
        "from nltk.tokenize import word_tokenize     #Imports the word_tokenize function from NLTK, which is used for splitting a sentence into individual words or tokens.\n",
        "import nltk             #Imports the main NLTK library to access other utilities like stopwords and tokenizers.\n",
        "nltk.download('punkt_tab')\n",
        "# Download NLTK stopwords (run only once)\n",
        "nltk.download('punkt')       #Downloads the Punkt tokenizer models, which are necessary for word tokenization (splitting sentences into words).\n",
        "nltk.download('stopwords')   #Downloads the stopwords list, which contains a set of common words in English that are typically removed from text before processing.\n",
        "# Sample documents\n",
        "documents = [\n",
        "    \"Artificial intelligence is transforming the technology industry.\",\n",
        "    \"Machine learning and AI are shaping the future of automation.\",\n",
        "    \"Deep learning algorithms are a subset of machine learning.\",\n",
        "    \"Quantum computing will revolutionize industries like AI.\",\n",
        "    \"Healthcare is benefiting from AI and machine learning advances.\",\n",
        "]\n",
        "#This defines a list of sample documents (sentences) that will be used for topic modeling. These sentences are focused on topics related to artificial intelligence (AI) and machine learning (ML).\n",
        "\n",
        "# Preprocess the documents\n",
        "def preprocess(doc):                                    #Defines a function that preprocesses a document (sentence) to prepare it for modeling by removing stopwords and non-alphabetic words.\n",
        "    stop_words = set(stopwords.words('english'))        #Loads the set of English stopwords from NLTK into the stop_words variable. These are words like \"and\", \"the\", \"is\", etc., that generally do not carry important meaning for topic modeling.\n",
        "    tokens = word_tokenize(doc.lower())                 #Tokenizes the input doc (document) into individual words (tokens) and converts all words to lowercase using .lower() to ensure uniformity (e.g., \"AI\" and \"ai\" will be treated as the same).\n",
        "    return [word for word in tokens if word.isalpha() and word not in stop_words]    #Filters out any tokens that are non-alphabetic (such as punctuation or numbers) and any stopwords. It returns a list of meaningful words (tokens).\n",
        "\n",
        "processed_docs = [preprocess(doc) for doc in documents]               #Applies the preprocess() function to each document in the documents list. This results in a list of tokenized, lowercased, stopword-free words for each document.\n",
        "\n",
        "# Create a dictionary and document-term matrix\n",
        "dictionary = corpora.Dictionary(processed_docs)                          #Creates a dictionary using the processed documents. The dictionary maps each unique word (token) to a unique ID. This is an essential step before building a document-term matrix (DTM).\n",
        "doc_term_matrix = [dictionary.doc2bow(doc) for doc in processed_docs]    #Converts each preprocessed document into a bag-of-words representation using dictionary.doc2bow(). The doc2bow function converts each document into a list of tuples, where each tuple represents a word ID and its frequency in the document.\n",
        "\n",
        "# Train the LDA model (specifying 2 topics)\n",
        "lda_model = LdaModel(doc_term_matrix, num_topics=2, id2word=dictionary, passes=15)   #Specifies that the model should discover 2 topics from the documents. id2word=dictionary: The dictionary created earlier is passed to the model to help interpret word IDs.. passes=15: Specifies the number of passes (iterations) over the entire corpus to optimize the model. More passes generally result in better topic quality but take more time.\n",
        "\n",
        "# Print the topics with associated words\n",
        "print(\"Topics discovered by LDA:\")\n",
        "topics = lda_model.print_topics(num_words=5)    #Prints the top 5 words for each discovered topic. This allows you to understand what each topic is about based on the most common words in the topic.\n",
        "for topic in topics:\n",
        "    print(topic)        #Iterates over the topics and prints them out. Each topic consists of a list of words that are highly associated with that topic.\n",
        "\n",
        "# Document similarity (clustering example)\n",
        "doc1_bow = dictionary.doc2bow(preprocess(\"AI and machine learning are advancing rapidly\"))   #Preprocesses the new document, converts it into a bag-of-words format using the dictionary, and stores it in doc1_bow.\n",
        "doc2_bow = dictionary.doc2bow(preprocess(\"Healthcare is benefiting from AI advances\"))       #Preprocesses and converts the second document into a bag-of-words representation, storing it in doc2_bow.\n",
        "\n",
        "similarity = gensim.matutils.cossim(doc1_bow, doc2_bow)   #Computes the cosine similarity between the two document vectors (doc1_bow and doc2_bow). Cosine similarity is a measure of similarity between two vectors based on the cosine of the angle between them. A higher cosine value indicates more similarity.\n",
        "print(\"\\nDocument Similarity (cosine):\", similarity)   #Prints the cosine similarity value between the two documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "w5vV4Q3h2iB6",
        "outputId": "7ce990da-3534-4299-fff8-627983112a47"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.14.1\n",
            "    Uninstalling scipy-1.14.1:\n",
            "      Successfully uninstalled scipy-1.14.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'numpy.rec'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0575a766ec4d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install gensim'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m                               \u001b[0;31m#Imports the gensim library, which is a popular Python library for topic modeling, document similarity, and vector space modeling. It includes the LDA model, which we’ll use for topic modeling.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcorpora\u001b[0m                  \u001b[0;31m#Imports the corpora module from Gensim, which provides utilities for handling a corpus of documents. It includes methods for creating a dictionary (mapping words to unique IDs) and for creating document-term matrices.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLdaModel\u001b[0m          \u001b[0;31m#Imports the LdaModel class from Gensim. This class is used for training an LDA model on a corpus to discover topics within a collection of documents.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m           \u001b[0;31m#Imports the stopwords corpus from NLTK (Natural Language Toolkit), which contains a list of common words (e.g., \"the\", \"is\", \"and\") that are often removed from text during preprocessing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparsing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/parsing/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mporter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m from .preprocessing import (  # noqa:F401\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mpreprocess_documents\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mpreprocess_string\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/parsing/preprocessing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mporter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msmart_open\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/sparse/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_warnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_csr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_csc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/sparse/_base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVisibleDeprecationWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m from ._sputils import (asmatrix, check_reshape_kwargs, check_shape,\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/_lib/_util.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_array_api\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray_namespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/_lib/_array_api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray_api_compat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m from scipy._lib.array_api_compat import (\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mis_array_api_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/_lib/array_api_compat/numpy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# from numpy import * doesn't overwrite these builtin names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfloat32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy.rec'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}